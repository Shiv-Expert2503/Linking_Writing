{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:42:38.544652600Z",
     "start_time": "2023-12-27T06:42:36.868953200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.abspath(os.path.join(os.getcwd(),'..','artifacts','train_logs.csv')))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:42:55.494104400Z",
     "start_time": "2023-12-27T06:42:39.656885900Z"
    }
   },
   "id": "a42db9ab29008dc9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "               id  event_id  down_time  up_time  action_time       activity  \\\n0        001519c8         1       4526     4557           31  Nonproduction   \n1        001519c8         2       4558     4962          404  Nonproduction   \n2        001519c8         3     106571   106571            0  Nonproduction   \n3        001519c8         4     106686   106777           91          Input   \n4        001519c8         5     107196   107323          127          Input   \n...           ...       ...        ...      ...          ...            ...   \n8405893  fff05981      3615    2063944  2064440          496  Nonproduction   \n8405894  fff05981      3616    2064497  2064497            0  Nonproduction   \n8405895  fff05981      3617    2064657  2064765          108        Replace   \n8405896  fff05981      3618    2069186  2069259           73  Nonproduction   \n8405897  fff05981      3619    2070065  2070133           68          Input   \n\n        down_event   up_event text_change  cursor_position  word_count  \n0        Leftclick  Leftclick    NoChange                0           0  \n1        Leftclick  Leftclick    NoChange                0           0  \n2            Shift      Shift    NoChange                0           0  \n3                q          q           q                1           1  \n4                q          q           q                2           1  \n...            ...        ...         ...              ...         ...  \n8405893  Leftclick  Leftclick    NoChange             1031         240  \n8405894      Shift      Shift    NoChange             1031         240  \n8405895          q          q      q => q             1031         240  \n8405896  Leftclick  Leftclick    NoChange             1028         240  \n8405897          .          .           .             1029         240  \n\n[8405898 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>event_id</th>\n      <th>down_time</th>\n      <th>up_time</th>\n      <th>action_time</th>\n      <th>activity</th>\n      <th>down_event</th>\n      <th>up_event</th>\n      <th>text_change</th>\n      <th>cursor_position</th>\n      <th>word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001519c8</td>\n      <td>1</td>\n      <td>4526</td>\n      <td>4557</td>\n      <td>31</td>\n      <td>Nonproduction</td>\n      <td>Leftclick</td>\n      <td>Leftclick</td>\n      <td>NoChange</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>001519c8</td>\n      <td>2</td>\n      <td>4558</td>\n      <td>4962</td>\n      <td>404</td>\n      <td>Nonproduction</td>\n      <td>Leftclick</td>\n      <td>Leftclick</td>\n      <td>NoChange</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>001519c8</td>\n      <td>3</td>\n      <td>106571</td>\n      <td>106571</td>\n      <td>0</td>\n      <td>Nonproduction</td>\n      <td>Shift</td>\n      <td>Shift</td>\n      <td>NoChange</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>001519c8</td>\n      <td>4</td>\n      <td>106686</td>\n      <td>106777</td>\n      <td>91</td>\n      <td>Input</td>\n      <td>q</td>\n      <td>q</td>\n      <td>q</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>001519c8</td>\n      <td>5</td>\n      <td>107196</td>\n      <td>107323</td>\n      <td>127</td>\n      <td>Input</td>\n      <td>q</td>\n      <td>q</td>\n      <td>q</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8405893</th>\n      <td>fff05981</td>\n      <td>3615</td>\n      <td>2063944</td>\n      <td>2064440</td>\n      <td>496</td>\n      <td>Nonproduction</td>\n      <td>Leftclick</td>\n      <td>Leftclick</td>\n      <td>NoChange</td>\n      <td>1031</td>\n      <td>240</td>\n    </tr>\n    <tr>\n      <th>8405894</th>\n      <td>fff05981</td>\n      <td>3616</td>\n      <td>2064497</td>\n      <td>2064497</td>\n      <td>0</td>\n      <td>Nonproduction</td>\n      <td>Shift</td>\n      <td>Shift</td>\n      <td>NoChange</td>\n      <td>1031</td>\n      <td>240</td>\n    </tr>\n    <tr>\n      <th>8405895</th>\n      <td>fff05981</td>\n      <td>3617</td>\n      <td>2064657</td>\n      <td>2064765</td>\n      <td>108</td>\n      <td>Replace</td>\n      <td>q</td>\n      <td>q</td>\n      <td>q =&gt; q</td>\n      <td>1031</td>\n      <td>240</td>\n    </tr>\n    <tr>\n      <th>8405896</th>\n      <td>fff05981</td>\n      <td>3618</td>\n      <td>2069186</td>\n      <td>2069259</td>\n      <td>73</td>\n      <td>Nonproduction</td>\n      <td>Leftclick</td>\n      <td>Leftclick</td>\n      <td>NoChange</td>\n      <td>1028</td>\n      <td>240</td>\n    </tr>\n    <tr>\n      <th>8405897</th>\n      <td>fff05981</td>\n      <td>3619</td>\n      <td>2070065</td>\n      <td>2070133</td>\n      <td>68</td>\n      <td>Input</td>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>1029</td>\n      <td>240</td>\n    </tr>\n  </tbody>\n</table>\n<p>8405898 rows Ã— 11 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:42:55.592897200Z",
     "start_time": "2023-12-27T06:42:55.495255400Z"
    }
   },
   "id": "a734b144aafcc787"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(8405898, 11)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:42:55.627460800Z",
     "start_time": "2023-12-27T06:42:55.551220600Z"
    }
   },
   "id": "c172b99bb462ab4f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import re\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.stats import skew, kurtosis\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:42:59.533597700Z",
     "start_time": "2023-12-27T06:42:55.566947400Z"
    }
   },
   "id": "3d820d8b9927a5d3",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "num_cols = ['down_time', 'up_time', 'action_time', 'cursor_position', 'word_count']\n",
    "activities = ['Input', 'Remove/Cut', 'Nonproduction', 'Replace', 'Paste']\n",
    "events = ['q', 'Space', 'Backspace', 'Shift', 'ArrowRight', 'Leftclick', 'ArrowLeft', '.', ',', 'ArrowDown', 'ArrowUp', 'Enter', 'CapsLock', \"'\", 'Delete', 'Unidentified']\n",
    "text_changes = ['q', ' ', '.', ',', '\\n', \"'\", '\"', '-', '?', ';', '=', '/', '\\\\', ':']\n",
    "\n",
    "\n",
    "def count_by_values(df, colname, values):\n",
    "    fts = df.select(pl.col('id').unique(maintain_order=True))\n",
    "    for i, value in enumerate(values):\n",
    "        tmp_df = df.group_by('id').agg(pl.col(colname).is_in([value]).sum().alias(f'{colname}_{i}_cnt'))\n",
    "        fts  = fts.join(tmp_df, on='id', how='left')\n",
    "    return fts\n",
    "\n",
    "\n",
    "def dev_feats(df):\n",
    "\n",
    "    print(\"< Count by values features >\")\n",
    "\n",
    "    feats = count_by_values(df, 'activity', activities)\n",
    "    feats = feats.join(count_by_values(df, 'text_change', text_changes), on='id', how='left')\n",
    "    feats = feats.join(count_by_values(df, 'down_event', events), on='id', how='left')\n",
    "    feats = feats.join(count_by_values(df, 'up_event', events), on='id', how='left')\n",
    "\n",
    "    print(\"< Input words stats features >\")\n",
    "    temp = df.filter((~pl.col('text_change').str.contains('=>')) & (pl.col('text_change') != 'NoChange'))\n",
    "    temp = temp.group_by('id').agg(pl.col('text_change').str.concat('').str.extract_all(r'q+'))\n",
    "    temp = temp.with_columns(input_word_count = pl.col('text_change').list.lengths(),\n",
    "                             input_word_length_mean = pl.col('text_change').apply(lambda x: np.mean([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_max = pl.col('text_change').apply(lambda x: np.max([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_std = pl.col('text_change').apply(lambda x: np.std([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_median = pl.col('text_change').apply(lambda x: np.median([len(i) for i in x] if len(x) > 0 else 0)),\n",
    "                             input_word_length_skew = pl.col('text_change').apply(lambda x: skew([len(i) for i in x] if len(x) > 0 else 0)))\n",
    "    temp = temp.drop('text_change')\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"< Numerical columns features >\")\n",
    "\n",
    "    temp = df.group_by(\"id\").agg(pl.sum('action_time').suffix('_sum'), pl.mean(num_cols).suffix('_mean'), pl.std(num_cols).suffix('_std'),\n",
    "                                 pl.median(num_cols).suffix('_median'), pl.min(num_cols).suffix('_min'), pl.max(num_cols).suffix('_max'),\n",
    "                                 pl.quantile(num_cols, 0.5).suffix('_quantile'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "\n",
    "    print(\"< Categorical columns features >\")\n",
    "\n",
    "    temp  = df.group_by(\"id\").agg(pl.n_unique(['activity', 'down_event', 'up_event', 'text_change']))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "    print(\"< Idle time features >\")\n",
    "\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.group_by(\"id\").agg(inter_key_largest_lantency = pl.max('time_diff'),\n",
    "                                   inter_key_median_lantency = pl.median('time_diff'),\n",
    "                                   mean_pause_time = pl.mean('time_diff'),\n",
    "                                   std_pause_time = pl.std('time_diff'),\n",
    "                                   total_pause_time = pl.sum('time_diff'),\n",
    "                                   pauses_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 0.5) & (pl.col('time_diff') < 1)).count(),\n",
    "                                   pauses_1_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1) & (pl.col('time_diff') < 1.5)).count(),\n",
    "                                   pauses_1_half_sec = pl.col('time_diff').filter((pl.col('time_diff') > 1.5) & (pl.col('time_diff') < 2)).count(),\n",
    "                                   pauses_2_sec = pl.col('time_diff').filter((pl.col('time_diff') > 2) & (pl.col('time_diff') < 3)).count(),\n",
    "                                   pauses_3_sec = pl.col('time_diff').filter(pl.col('time_diff') > 3).count(),)\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "    print(\"< P-bursts features >\")\n",
    "    temp = df.with_columns(pl.col('up_time').shift().over('id').alias('up_time_lagged'))\n",
    "    temp = temp.with_columns((abs(pl.col('down_time') - pl.col('up_time_lagged')) / 1000).fill_null(0).alias('time_diff'))\n",
    "    temp = temp.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('time_diff')<2)\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"time_diff\") & pl.col(\"time_diff\").is_last()).then(pl.count()).over(pl.col(\"time_diff\").rle_id()).alias('P-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('P-bursts').suffix('_mean'), pl.std('P-bursts').suffix('_std'), pl.count('P-bursts').suffix('_count'),\n",
    "                                   pl.median('P-bursts').suffix('_median'), pl.max('P-bursts').suffix('_max'),\n",
    "                                   pl.first('P-bursts').suffix('_first'), pl.last('P-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "\n",
    "    print(\"< R-bursts features >\")\n",
    "\n",
    "    temp = df.filter(pl.col('activity').is_in(['Input', 'Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.col('activity').is_in(['Remove/Cut']))\n",
    "    temp = temp.with_columns(pl.when(pl.col(\"activity\") & pl.col(\"activity\").is_last()).then(pl.count()).over(pl.col(\"activity\").rle_id()).alias('R-bursts'))\n",
    "    temp = temp.drop_nulls()\n",
    "    temp = temp.group_by(\"id\").agg(pl.mean('R-bursts').suffix('_mean'), pl.std('R-bursts').suffix('_std'),\n",
    "                                   pl.median('R-bursts').suffix('_median'), pl.max('R-bursts').suffix('_max'),\n",
    "                                   pl.first('R-bursts').suffix('_first'), pl.last('R-bursts').suffix('_last'))\n",
    "    feats = feats.join(temp, on='id', how='left')\n",
    "\n",
    "    return feats\n",
    "\n",
    "def train_valid_split(data_x, data_y, train_idx, valid_idx):\n",
    "    x_train = data_x.iloc[train_idx]\n",
    "    y_train = data_y[train_idx]\n",
    "    x_valid = data_x.iloc[valid_idx]\n",
    "    y_valid = data_y[valid_idx]\n",
    "    return x_train, y_train, x_valid, y_valid\n",
    "\n",
    "\n",
    "def evaluate(data_x, data_y, model, random_state=42, n_splits=5, test_x=None):\n",
    "    skf    = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "    test_y = np.zeros(len(data_x)) if (test_x is None) else np.zeros((len(test_x), n_splits))\n",
    "    for i, (train_index, valid_index) in enumerate(skf.split(data_x, data_y.astype(str))):\n",
    "        train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n",
    "        model.fit(train_x, train_y)\n",
    "        if test_x is None:\n",
    "            test_y[valid_index] = model.predict(valid_x)\n",
    "        else:\n",
    "            test_y[:, i] = model.predict(test_x)\n",
    "    return test_y if (test_x is None) else np.mean(test_y, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:42:59.598856700Z",
     "start_time": "2023-12-27T06:42:59.571682400Z"
    }
   },
   "id": "74b1de09fbb031ac",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    return x.quantile(0.25)\n",
    "def q3(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "AGGREGATIONS = ['count', 'mean', 'min', 'max', 'first', 'last', q1, 'median', q3, 'sum']\n",
    "\n",
    "def reconstruct_essay(currTextInput):\n",
    "    essayText = \"\"\n",
    "    for Input in currTextInput.values:\n",
    "        if Input[0] == 'Replace':\n",
    "            replaceTxt = Input[2].split(' => ')\n",
    "            essayText = essayText[:Input[1] - len(replaceTxt[1])] + replaceTxt[1] + essayText[Input[1] - len(replaceTxt[1]) + len(replaceTxt[0]):]\n",
    "            continue\n",
    "        if Input[0] == 'Paste':\n",
    "            essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "            continue\n",
    "        if Input[0] == 'Remove/Cut':\n",
    "            essayText = essayText[:Input[1]] + essayText[Input[1] + len(Input[2]):]\n",
    "            continue\n",
    "        if \"M\" in Input[0]:\n",
    "            croppedTxt = Input[0][10:]\n",
    "            splitTxt = croppedTxt.split(' To ')\n",
    "            valueArr = [item.split(', ') for item in splitTxt]\n",
    "            moveData = (int(valueArr[0][0][1:]), int(valueArr[0][1][:-1]), int(valueArr[1][0][1:]), int(valueArr[1][1][:-1]))\n",
    "            if moveData[0] != moveData[2]:\n",
    "                if moveData[0] < moveData[2]:\n",
    "                    essayText = essayText[:moveData[0]] + essayText[moveData[1]:moveData[3]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[3]:]\n",
    "                else:\n",
    "                    essayText = essayText[:moveData[2]] + essayText[moveData[0]:moveData[1]] + essayText[moveData[2]:moveData[0]] + essayText[moveData[1]:]\n",
    "            continue\n",
    "        essayText = essayText[:Input[1] - len(Input[2])] + Input[2] + essayText[Input[1] - len(Input[2]):]\n",
    "    return essayText\n",
    "\n",
    "def get_essay_df(df):\n",
    "    df       = df[df.activity != 'Nonproduction']\n",
    "    temp     = df.groupby('id').apply(lambda x: reconstruct_essay(x[['activity', 'cursor_position', 'text_change']]))\n",
    "    essay_df = pd.DataFrame({'id': df['id'].unique().tolist()})\n",
    "    essay_df = essay_df.merge(temp.rename('essay'), on='id')\n",
    "    return essay_df\n",
    "\n",
    "\n",
    "def word_feats(df):\n",
    "    essay_df = df\n",
    "    df['word'] = df['essay'].apply(lambda x: re.split(' |\\\\n|\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('word')\n",
    "    df['word_len'] = df['word'].apply(lambda x: len(x))\n",
    "    df = df[df['word_len'] != 0]\n",
    "\n",
    "    word_agg_df = df[['id','word_len']].groupby(['id']).agg(AGGREGATIONS)\n",
    "    word_agg_df.columns = ['_'.join(x) for x in word_agg_df.columns]\n",
    "    word_agg_df['id'] = word_agg_df.index\n",
    "    word_agg_df = word_agg_df.reset_index(drop=True)\n",
    "    return word_agg_df\n",
    "\n",
    "def sent_feats(df):\n",
    "    df['sent'] = df['essay'].apply(lambda x: re.split('\\\\.|\\\\?|\\\\!',x))\n",
    "    df = df.explode('sent')\n",
    "    df['sent'] = df['sent'].apply(lambda x: x.replace('\\n','').strip())\n",
    "    # Number of characters in sentences\n",
    "    df['sent_len'] = df['sent'].apply(lambda x: len(x))\n",
    "    # Number of words in sentences\n",
    "    df['sent_word_count'] = df['sent'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.sent_len!=0].reset_index(drop=True)\n",
    "\n",
    "    sent_agg_df = pd.concat([df[['id','sent_len']].groupby(['id']).agg(AGGREGATIONS),\n",
    "                             df[['id','sent_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    sent_agg_df.columns = ['_'.join(x) for x in sent_agg_df.columns]\n",
    "    sent_agg_df['id'] = sent_agg_df.index\n",
    "    sent_agg_df = sent_agg_df.reset_index(drop=True)\n",
    "    sent_agg_df.drop(columns=[\"sent_word_count_count\"], inplace=True)\n",
    "    sent_agg_df = sent_agg_df.rename(columns={\"sent_len_count\":\"sent_count\"})\n",
    "    return sent_agg_df\n",
    "\n",
    "def parag_feats(df):\n",
    "    df['paragraph'] = df['essay'].apply(lambda x: x.split('\\n'))\n",
    "    df = df.explode('paragraph')\n",
    "    # Number of characters in paragraphs\n",
    "    df['paragraph_len'] = df['paragraph'].apply(lambda x: len(x))\n",
    "    # Number of words in paragraphs\n",
    "    df['paragraph_word_count'] = df['paragraph'].apply(lambda x: len(x.split(' ')))\n",
    "    df = df[df.paragraph_len!=0].reset_index(drop=True)\n",
    "\n",
    "    paragraph_agg_df = pd.concat([df[['id','paragraph_len']].groupby(['id']).agg(AGGREGATIONS),\n",
    "                                  df[['id','paragraph_word_count']].groupby(['id']).agg(AGGREGATIONS)], axis=1)\n",
    "    paragraph_agg_df.columns = ['_'.join(x) for x in paragraph_agg_df.columns]\n",
    "    paragraph_agg_df['id'] = paragraph_agg_df.index\n",
    "    paragraph_agg_df = paragraph_agg_df.reset_index(drop=True)\n",
    "    paragraph_agg_df.drop(columns=[\"paragraph_word_count_count\"], inplace=True)\n",
    "    paragraph_agg_df = paragraph_agg_df.rename(columns={\"paragraph_len_count\":\"paragraph_count\"})\n",
    "    return paragraph_agg_df\n",
    "\n",
    "def product_to_keys(logs, essays):\n",
    "    essays['product_len'] = essays.essay.str.len()\n",
    "    tmp_df = logs[logs.activity.isin(['Input', 'Remove/Cut'])].groupby(['id']).agg({'activity': 'count'}).reset_index().rename(columns={'activity': 'keys_pressed'})\n",
    "    essays = essays.merge(tmp_df, on='id', how='left')\n",
    "    essays['product_to_keys'] = essays['product_len'] / essays['keys_pressed']\n",
    "    return essays[['id', 'product_to_keys']]\n",
    "\n",
    "def get_keys_pressed_per_second(logs):\n",
    "    temp_df = logs[logs['activity'].isin(['Input', 'Remove/Cut'])].groupby(['id']).agg(keys_pressed=('event_id', 'count')).reset_index()\n",
    "    temp_df_2 = logs.groupby(['id']).agg(min_down_time=('down_time', 'min'), max_up_time=('up_time', 'max')).reset_index()\n",
    "    temp_df = temp_df.merge(temp_df_2, on='id', how='left')\n",
    "    temp_df['keys_per_second'] = temp_df['keys_pressed'] / ((temp_df['max_up_time'] - temp_df['min_down_time']) / 1000)\n",
    "    return temp_df[['id', 'keys_per_second']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:42:59.675610800Z",
     "start_time": "2023-12-27T06:42:59.615264500Z"
    }
   },
   "id": "d562a82438aa3379",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n",
      "< Essay Reconstruction >\n",
      "< Mapping >\n",
      "Number of features: 165\n",
      "< Testing Data >\n",
      "< Count by values features >\n",
      "< Input words stats features >\n",
      "< Numerical columns features >\n",
      "< Categorical columns features >\n",
      "< Idle time features >\n",
      "< P-bursts features >\n",
      "< R-bursts features >\n"
     ]
    }
   ],
   "source": [
    "# data_path     = os.path.abspath(os.path.join(os.getcwd(),'..','artifacts','train_logs.csv'))\n",
    "train_logs    = pl.scan_csv(os.path.abspath(os.path.join(os.getcwd(),'..','artifacts','train_logs.csv')))\n",
    "train_feats   = dev_feats(train_logs)\n",
    "train_feats   = train_feats.collect().to_pandas()\n",
    "\n",
    "print('< Essay Reconstruction >')\n",
    "train_logs             = train_logs.collect().to_pandas()\n",
    "train_essays           = get_essay_df(train_logs)\n",
    "train_feats            = train_feats.merge(word_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(sent_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(parag_feats(train_essays), on='id', how='left')\n",
    "train_feats            = train_feats.merge(get_keys_pressed_per_second(train_logs), on='id', how='left')\n",
    "train_feats            = train_feats.merge(product_to_keys(train_logs, train_essays), on='id', how='left')\n",
    "\n",
    "\n",
    "print('< Mapping >')\n",
    "train_scores   = pl.scan_csv(os.path.abspath(os.path.join(os.getcwd(),'..','artifacts','train_scores.csv')))\n",
    "train_scores   = train_scores.collect().to_pandas()\n",
    "data           = train_feats.merge(train_scores, on='id', how='left')\n",
    "x              = data.drop(['id', 'score'], axis=1)\n",
    "y              = data['score'].values\n",
    "print(f'Number of features: {len(x.columns)}')\n",
    "\n",
    "print('< Testing Data >')\n",
    "test_logs   = pl.scan_csv(os.path.abspath(os.path.join(os.getcwd(),'..','artifacts','test_logs.csv')))\n",
    "test_feats  = dev_feats(test_logs)\n",
    "test_feats  = test_feats.collect().to_pandas()\n",
    "\n",
    "test_logs             = test_logs.collect().to_pandas()\n",
    "test_essays           = get_essay_df(test_logs)\n",
    "test_feats            = test_feats.merge(word_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(sent_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(parag_feats(test_essays), on='id', how='left')\n",
    "test_feats            = test_feats.merge(get_keys_pressed_per_second(test_logs), on='id', how='left')\n",
    "test_feats            = test_feats.merge(product_to_keys(test_logs, test_essays), on='id', how='left')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:30.585367Z",
     "start_time": "2023-12-27T06:42:59.656079700Z"
    }
   },
   "id": "24cce52d5cd7176b",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "            id  activity_0_cnt  activity_1_cnt  activity_2_cnt  \\\n0     001519c8            2010             417             120   \n1     0022f953            1938             260             254   \n2     0042269b            3515             439             175   \n3     0059420b            1304             151              99   \n4     0075873a            1942             517              72   \n...        ...             ...             ...             ...   \n2466  ffb8c745            3588             960             189   \n2467  ffbef7e5            2395              60             148   \n2468  ffccd6fd            2849              88             126   \n2469  ffec5b38            2895             276              71   \n2470  fff05981            2452             310             843   \n\n      activity_3_cnt  activity_4_cnt  text_change_0_cnt  text_change_1_cnt  \\\n0                  7               0               1940                436   \n1                  1               1               1698                432   \n2                  7               0               3257                615   \n3                  1               1               1146                281   \n4                  0               0               1964                397   \n...              ...             ...                ...                ...   \n2466               2               0               3605                813   \n2467               1               0               1920                457   \n2468               0               0               1031               1879   \n2469               0               0               2593                490   \n2470              12               1               2177                474   \n\n      text_change_2_cnt  text_change_3_cnt  ...  paragraph_word_count_max  \\\n0                    28                 14  ...                       112   \n1                    18                 24  ...                        96   \n2                    23                 26  ...                        88   \n3                    13                  3  ...                        81   \n4                    32                 25  ...                       114   \n...                 ...                ...  ...                       ...   \n2466                 59                 42  ...                        88   \n2467                 33                 24  ...                       119   \n2468                  6                  3  ...                      1703   \n2469                 34                 29  ...                       111   \n2470                 17                 26  ...                        66   \n\n      paragraph_word_count_first  paragraph_word_count_last  \\\n0                             71                         86   \n1                             53                         60   \n2                             79                         45   \n3                             62                         65   \n4                             61                          3   \n...                          ...                        ...   \n2466                          71                         63   \n2467                          27                         40   \n2468                          83                       1703   \n2469                         111                         62   \n2470                          10                         38   \n\n      paragraph_word_count_q1  paragraph_word_count_median  \\\n0                       78.50                         86.0   \n1                       47.75                         56.5   \n2                       55.50                         73.5   \n3                       63.50                         65.0   \n4                       26.00                         52.0   \n...                       ...                          ...   \n2466                    69.00                         78.5   \n2467                    50.00                         83.5   \n2468                    71.50                         83.0   \n2469                    66.00                         85.0   \n2470                    25.25                         41.5   \n\n      paragraph_word_count_q3  paragraph_word_count_sum  keys_per_second  \\\n0                       99.00                       269         1.350251   \n1                       62.25                       355         1.250038   \n2                       78.75                       410         2.237402   \n3                       73.00                       208         1.067440   \n4                       61.00                       256         1.552397   \n...                       ...                       ...              ...   \n2466                    86.50                       308         2.570680   \n2467                    89.25                       443         1.381198   \n2468                   893.00                      1846         1.517139   \n2469                    93.00                       417         2.130162   \n2470                    60.00                       245         1.360319   \n\n      product_to_keys  score  \n0            0.629584    3.5  \n1            0.762056    3.5  \n2            0.654274    6.0  \n3            0.793127    2.0  \n4            0.579504    4.0  \n...               ...    ...  \n2466         0.359279    3.5  \n2467         0.951120    4.0  \n2468         0.940075    1.5  \n2469         0.804793    5.0  \n2470         0.540188    4.0  \n\n[2471 rows x 167 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>activity_0_cnt</th>\n      <th>activity_1_cnt</th>\n      <th>activity_2_cnt</th>\n      <th>activity_3_cnt</th>\n      <th>activity_4_cnt</th>\n      <th>text_change_0_cnt</th>\n      <th>text_change_1_cnt</th>\n      <th>text_change_2_cnt</th>\n      <th>text_change_3_cnt</th>\n      <th>...</th>\n      <th>paragraph_word_count_max</th>\n      <th>paragraph_word_count_first</th>\n      <th>paragraph_word_count_last</th>\n      <th>paragraph_word_count_q1</th>\n      <th>paragraph_word_count_median</th>\n      <th>paragraph_word_count_q3</th>\n      <th>paragraph_word_count_sum</th>\n      <th>keys_per_second</th>\n      <th>product_to_keys</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>001519c8</td>\n      <td>2010</td>\n      <td>417</td>\n      <td>120</td>\n      <td>7</td>\n      <td>0</td>\n      <td>1940</td>\n      <td>436</td>\n      <td>28</td>\n      <td>14</td>\n      <td>...</td>\n      <td>112</td>\n      <td>71</td>\n      <td>86</td>\n      <td>78.50</td>\n      <td>86.0</td>\n      <td>99.00</td>\n      <td>269</td>\n      <td>1.350251</td>\n      <td>0.629584</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0022f953</td>\n      <td>1938</td>\n      <td>260</td>\n      <td>254</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1698</td>\n      <td>432</td>\n      <td>18</td>\n      <td>24</td>\n      <td>...</td>\n      <td>96</td>\n      <td>53</td>\n      <td>60</td>\n      <td>47.75</td>\n      <td>56.5</td>\n      <td>62.25</td>\n      <td>355</td>\n      <td>1.250038</td>\n      <td>0.762056</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0042269b</td>\n      <td>3515</td>\n      <td>439</td>\n      <td>175</td>\n      <td>7</td>\n      <td>0</td>\n      <td>3257</td>\n      <td>615</td>\n      <td>23</td>\n      <td>26</td>\n      <td>...</td>\n      <td>88</td>\n      <td>79</td>\n      <td>45</td>\n      <td>55.50</td>\n      <td>73.5</td>\n      <td>78.75</td>\n      <td>410</td>\n      <td>2.237402</td>\n      <td>0.654274</td>\n      <td>6.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0059420b</td>\n      <td>1304</td>\n      <td>151</td>\n      <td>99</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1146</td>\n      <td>281</td>\n      <td>13</td>\n      <td>3</td>\n      <td>...</td>\n      <td>81</td>\n      <td>62</td>\n      <td>65</td>\n      <td>63.50</td>\n      <td>65.0</td>\n      <td>73.00</td>\n      <td>208</td>\n      <td>1.067440</td>\n      <td>0.793127</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0075873a</td>\n      <td>1942</td>\n      <td>517</td>\n      <td>72</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1964</td>\n      <td>397</td>\n      <td>32</td>\n      <td>25</td>\n      <td>...</td>\n      <td>114</td>\n      <td>61</td>\n      <td>3</td>\n      <td>26.00</td>\n      <td>52.0</td>\n      <td>61.00</td>\n      <td>256</td>\n      <td>1.552397</td>\n      <td>0.579504</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2466</th>\n      <td>ffb8c745</td>\n      <td>3588</td>\n      <td>960</td>\n      <td>189</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3605</td>\n      <td>813</td>\n      <td>59</td>\n      <td>42</td>\n      <td>...</td>\n      <td>88</td>\n      <td>71</td>\n      <td>63</td>\n      <td>69.00</td>\n      <td>78.5</td>\n      <td>86.50</td>\n      <td>308</td>\n      <td>2.570680</td>\n      <td>0.359279</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>2467</th>\n      <td>ffbef7e5</td>\n      <td>2395</td>\n      <td>60</td>\n      <td>148</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1920</td>\n      <td>457</td>\n      <td>33</td>\n      <td>24</td>\n      <td>...</td>\n      <td>119</td>\n      <td>27</td>\n      <td>40</td>\n      <td>50.00</td>\n      <td>83.5</td>\n      <td>89.25</td>\n      <td>443</td>\n      <td>1.381198</td>\n      <td>0.951120</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2468</th>\n      <td>ffccd6fd</td>\n      <td>2849</td>\n      <td>88</td>\n      <td>126</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1031</td>\n      <td>1879</td>\n      <td>6</td>\n      <td>3</td>\n      <td>...</td>\n      <td>1703</td>\n      <td>83</td>\n      <td>1703</td>\n      <td>71.50</td>\n      <td>83.0</td>\n      <td>893.00</td>\n      <td>1846</td>\n      <td>1.517139</td>\n      <td>0.940075</td>\n      <td>1.5</td>\n    </tr>\n    <tr>\n      <th>2469</th>\n      <td>ffec5b38</td>\n      <td>2895</td>\n      <td>276</td>\n      <td>71</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2593</td>\n      <td>490</td>\n      <td>34</td>\n      <td>29</td>\n      <td>...</td>\n      <td>111</td>\n      <td>111</td>\n      <td>62</td>\n      <td>66.00</td>\n      <td>85.0</td>\n      <td>93.00</td>\n      <td>417</td>\n      <td>2.130162</td>\n      <td>0.804793</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>2470</th>\n      <td>fff05981</td>\n      <td>2452</td>\n      <td>310</td>\n      <td>843</td>\n      <td>12</td>\n      <td>1</td>\n      <td>2177</td>\n      <td>474</td>\n      <td>17</td>\n      <td>26</td>\n      <td>...</td>\n      <td>66</td>\n      <td>10</td>\n      <td>38</td>\n      <td>25.25</td>\n      <td>41.5</td>\n      <td>60.00</td>\n      <td>245</td>\n      <td>1.360319</td>\n      <td>0.540188</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2471 rows Ã— 167 columns</p>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:30.653848700Z",
     "start_time": "2023-12-27T06:44:30.593091400Z"
    }
   },
   "id": "4526c4e3184a8891",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:30.678995800Z",
     "start_time": "2023-12-27T06:44:30.637247Z"
    }
   },
   "id": "61c33afc30bc147b",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:30.698876400Z",
     "start_time": "2023-12-27T06:44:30.649836Z"
    }
   },
   "id": "678058406c7b72c6",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:30.727566400Z",
     "start_time": "2023-12-27T06:44:30.660258600Z"
    }
   },
   "id": "c8cef625579f32c6",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:30.733768500Z",
     "start_time": "2023-12-27T06:44:30.668108600Z"
    }
   },
   "id": "29fc6ad3377406f",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:30.792387400Z",
     "start_time": "2023-12-27T06:44:30.676982800Z"
    }
   },
   "id": "654c7956c0f91de3",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "#RFE and PCA for feature selection\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from src.logger import logging\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor  # Change from Classifier to Regressor\n",
    "from catboost import CatBoostRegressor  # Change from Classifier to Regressor\n",
    "from lightgbm import LGBMRegressor  # Change from Classifier to Regressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:31.881630700Z",
     "start_time": "2023-12-27T06:44:30.687013100Z"
    }
   },
   "id": "13e784f670dd2843"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "xgb_best = XGBRegressor()\n",
    "lr_best = LinearRegression()\n",
    "cat_best = CatBoostRegressor(depth= 5, l2_leaf_reg= 5, learning_rate= 0.1, n_estimators= 100)\n",
    "lgbm_best = LGBMRegressor(learning_rate= 0.1, max_bin=511, max_depth= 5, min_child_samples= 30, n_estimators= 50, num_leaves= 31)\n",
    "svr_regressor = SVR(C= 1, gamma= 'auto', kernel= 'rbf')\n",
    "rforest_best = RandomForestRegressor(max_depth= 7, max_features= 'sqrt', max_leaf_nodes= 50, n_estimators= 200)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:31.922308700Z",
     "start_time": "2023-12-27T06:44:31.897831700Z"
    }
   },
   "id": "6510df7d726993e2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def handle_null(df):\n",
    "    missing_columns = df.columns[df.isna().sum() > 0]\n",
    "    df[missing_columns] = df[missing_columns].fillna(0)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:31.927441500Z",
     "start_time": "2023-12-27T06:44:31.903973900Z"
    }
   },
   "id": "a2c4913427ec03b7",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df=handle_null(data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:31.978836800Z",
     "start_time": "2023-12-27T06:44:31.914277700Z"
    }
   },
   "id": "c6149f608e5fb528",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(df.drop(['id','score'],axis=1),df['score'],test_size = 0.2,random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:32.014774800Z",
     "start_time": "2023-12-27T06:44:31.941018600Z"
    }
   },
   "id": "2b878e51d53d9f8f",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_test_scaled = scaler.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:32.090701500Z",
     "start_time": "2023-12-27T06:44:31.970744600Z"
    }
   },
   "id": "fa87cfda8686675",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Splitting the scaled training set into training and validation sets\n",
    "X_train_scaled, X_valid_scaled, y_train, y_valid = train_test_split(x_train_scaled, y_train, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T06:44:32.092767500Z",
     "start_time": "2023-12-27T06:44:32.003961200Z"
    }
   },
   "id": "754d03361e62c386",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# List of base regressors with their respective best models\n",
    "base_regressors = [\n",
    "    # ('LGBM', lgbm_best, {   'learning_rate': [0.01, 0.1, 0.2],\n",
    "    #                         'n_estimators': [50, 100, 200],\n",
    "    #                         'max_depth': [5, 10, 15],\n",
    "    #                         'min_child_samples': [10, 20, 30],\n",
    "    #                         'num_leaves': [31, 63, 127],\n",
    "    #                         'max_bin': [255, 511, 1023]}),\n",
    "    # ('CatBoost', cat_best, { 'learning_rate': [0.01, 0.1, 0.2],\n",
    "    #                          'n_estimators': [50, 100, 200],\n",
    "    #                          'depth': [5, 10, 15],\n",
    "    #                          'l2_leaf_reg': [1, 3, 5]}),\n",
    "    ('RandomForest', rforest_best, {'n_estimators': [50, 100, 200],\n",
    "                                    'max_depth': [3, 5, 7],\n",
    "                                    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                                    'max_leaf_nodes': [10, 20, 30, 40, 50]}),\n",
    "    # ('SVR', svr_regressor, {'kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n",
    "    #                         'C': [1, 10, 100],\n",
    "    #                         'gamma': ['scale', 'auto']}),\n",
    "    ('XGBOOST', xgb_best, {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'lambda': [0.01, 0.1, 1.0],\n",
    "        'alpha': [0.01, 0.1, 1.0],\n",
    "        'scale_pos_weight': [1, 2, 5]\n",
    "    })\n",
    "]\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for regressor_name, base_regressor, param_grid in base_regressors:\n",
    "    logging.info(f\"Performing Grid Search for {regressor_name}...\")\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create the GridSearchCV object\n",
    "    grid_search = GridSearchCV(base_regressor, param_grid, scoring='neg_root_mean_squared_error', cv=5)\n",
    "\n",
    "    # Fit the model to the scaled training data\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Get the best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    results_dict[regressor_name] = {'best_params': best_params, 'training_time': end_time - start_time}\n",
    "\n",
    "    logging.info(f\"Best Parameters for {regressor_name}: {best_params}\")\n",
    "    logging.info(f\"Root Mean Squared Error on Validation Set: {np.sqrt(mean_squared_error(y_valid, grid_search.predict(X_valid_scaled)))}\")\n",
    "    logging.info(f\"Training Time: {end_time - start_time:.2f} seconds\")\n",
    "    print()\n",
    "\n",
    "# Display the best parameters and training times for each model\n",
    "logging.info(\"Best Parameters and Training Times for Each Model:\")\n",
    "for regressor_name, result in results_dict.items():\n",
    "    logging.info(f\"{regressor_name}: {result['best_params']}, Training Time: {result['training_time']:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-27T06:45:31.215462500Z"
    }
   },
   "id": "8ba191c739045d7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003209 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26485\n",
      "[LightGBM] [Info] Number of data points in the train set: 1976, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.709008\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002150 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 25963\n",
      "[LightGBM] [Info] Number of data points in the train set: 1580, number of used features: 160\n",
      "[LightGBM] [Info] Start training from score 3.713608\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26046\n",
      "[LightGBM] [Info] Number of data points in the train set: 1581, number of used features: 160\n",
      "[LightGBM] [Info] Start training from score 3.700506\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26021\n",
      "[LightGBM] [Info] Number of data points in the train set: 1581, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.713156\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002709 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26011\n",
      "[LightGBM] [Info] Number of data points in the train set: 1581, number of used features: 160\n",
      "[LightGBM] [Info] Start training from score 3.690386\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 26052\n",
      "[LightGBM] [Info] Number of data points in the train set: 1581, number of used features: 161\n",
      "[LightGBM] [Info] Start training from score 3.727388\n",
      "Stacking Test Set RMSE: 0.5401494713515053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "base_regressors = [\n",
    "    ('LGBM', lgbm_best),\n",
    "    ('CatBoost', cat_best),\n",
    "    ('RandomForest', rforest_best),\n",
    "#     ('SVR', svr_regressor)\n",
    "]\n",
    "\n",
    "\n",
    "stacking_regressor = StackingRegressor(estimators=base_regressors, final_estimator=LinearRegression())\n",
    "stacking_regressor.fit(x_train_scaled, y_train)\n",
    "\n",
    "stacking_predictions = stacking_regressor.predict(x_test_scaled)\n",
    "\n",
    "stacking_rmse = mean_squared_error(y_test, stacking_predictions, squared=False)\n",
    "print(\"Stacking Test Set RMSE:\", stacking_rmse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T06:15:52.274203600Z",
     "start_time": "2023-12-26T06:12:20.358075600Z"
    }
   },
   "id": "bafaf8cd3ecc1057"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "efeac010af439488"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
